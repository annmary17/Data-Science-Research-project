---
title: "Nlp_Project"
output: html_document
date: "2025-07-22"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This file downloads the book 'Around the World in 80 Daysâ€™ from Project Gutenberg, does basic cleaning of the book, and saves the data in a csv file

##### Author: Ann Mary Anish
##### Last Modified: July 2025


## 1. LOAD LIBRARIES

```{r}


pacman::p_load(tidyverse, textdata, tidytext, syuzhet, gutenbergr, textstem,
               stringr, ggrepel, dplyr, ggplot2, lexicon, grid)

```
## 2. DOWNLOAD BOOK FROM PROJECT GUTENBERG

```{r}

book <- gutenberg_download(103, mirror = "http://mirror.csclub.uwaterloo.ca/gutenberg/")
```
## 3. CLEAN TEXT AND SEPARATE INTO CHAPTERS
```{r}


#Remove Header and Footer 
book_text_only <- book %>%
  tail(nrow(book) - 53)  # Removes top header lines

## Separate Book by Chapters 
book_chapters <- book_text_only %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))
  ) %>%
  ungroup()

## Remove Empty Lines 
book_chapters_clean <- book_chapters %>%
  filter(text != "")

## Remove Chapter Headings 
ch <- 0
rows <- c()
for (i in 1:nrow(book_chapters_clean)) {
  temp_ch <- book_chapters_clean[i, "chapter"]
  if (!(temp_ch == ch)) {
    ch <- temp_ch
    rows <- c(rows, i)
  } else {
    if (str_detect(book_chapters_clean[i, "text"][[1]],
                   "^([[:upper:]]|[[:digit:]]|[[:punct:]]|[[:blank:]])+$")) {
      rows <- c(rows, i)
    }
  }
}
rows <- unique(rows)

# Remove those rows
book_chapters_clean2 <- book_chapters_clean %>%
  filter(!row_number() %in% rows)

## Combine Text by Chapter 
book_chapters_clean3 <- book_chapters_clean2 %>%
  select(chapter, text) %>%
  group_by(chapter) %>%
  summarise(text = paste(text, collapse = " "), .groups = "drop")

```

# 4. SAVE CLEANED CHAPTER DATA
```{r}

write.csv(book_chapters_clean3, "around_world_80_days.csv", row.names = FALSE)
```

# 5. TOKENISATION AND TEXT CLEANING
```{r}

data("stop_words")

book_tokens <- book_chapters_clean3 %>%
  mutate(text = str_to_lower(text),                              # Lowercase
         text = str_replace_all(text, "[[:punct:]]", " "),       # Remove punctuation
         text = str_replace_all(text, "[[:digit:]]", ""),        # Remove numbers
         text = str_replace_all(text, "\\s+", " ")) %>%          # Remove extra whitespace
  unnest_tokens(word, text) %>%                                  # Tokenisation
  anti_join(stop_words, by = "word") %>%                         # Remove stop words
  mutate(word = lemmatize_words(word))                           # Lemmatise

```

# 6. SAVE TOKENISED DATA
```{r}
## Save Preprocessed Tokens 
write.csv(book_tokens, "tokens_for_sentiment.csv", row.names = FALSE)

```

# 7. LOAD PREPROCESSED TOKENS AND COUNT UNIQUE WORDS
```{r}

tokens <- read.csv("tokens_for_sentiment.csv")
unique_words <- length(unique(book_tokens$word))

```

# 8. LOAD & PREPARE SENTIMENT LEXICONS
```{r}

# --- AFINN ---
afinn <- get_sentiments("afinn") %>%
  mutate(word = tolower(word))

# --- Bing ---
bing <- get_sentiments("bing") %>%
  mutate(word = tolower(word))

# --- NRC (only positive and negative) ---
nrc <- get_sentiments("nrc") %>%
  filter(sentiment %in% c("positive", "negative")) %>%
  distinct(word, sentiment, .keep_all = TRUE) %>%
  mutate(word = tolower(word))

# --- SenticNet ---
senticnet_lex <- lexicon::hash_sentiment_senticnet %>%
  transmute(word = tolower(x), sentiment = as.numeric(y))

# --- SentiWordNet ---
sentiword_lex <- lexicon::hash_sentiment_sentiword %>%
  transmute(word = tolower(x), sentiment = as.numeric(y))

```

# 9. CALCULATE LEXICON COVERAGE
```{r}

# Function to calculate coverage for a given lexicon
calc_coverage <- function(data, lexicon) {
  data %>%
    inner_join(lexicon, by = "word") %>%
    summarise(matched_words = n_distinct(word)) %>%
    mutate(coverage = matched_words / unique_words * 100)
}

# Coverage for each lexicon
afinn_coverage       <- calc_coverage(book_tokens, afinn)
bing_coverage        <- calc_coverage(book_tokens, bing)
nrc_coverage         <- calc_coverage(book_tokens, nrc)
sentiword_coverage   <- calc_coverage(book_tokens, sentiword_lex)
senticnet_coverage   <- calc_coverage(book_tokens, senticnet_lex)

# Combine all coverage results into a single dataframe
coverage_df <- bind_rows(
  AFINN        = afinn_coverage,
  Bing         = bing_coverage,
  NRC          = nrc_coverage,
  SentiWordNet = sentiword_coverage,
  SenticNet    = senticnet_coverage,
  .id = "Lexicon"
) %>%
  arrange(desc(coverage))

# View results
print(coverage_df)

# Save coverage data
write.csv(coverage_df, "sentiment_lexicon_coverage.csv", row.names = FALSE)

```

# 10. CALCULATE SENTIMENT PER CHAPTER
```{r}

#  SenticNet Sentiment per chapter 
sentic_sentiment <- book_tokens %>%
  inner_join(senticnet_lex, by = "word") %>%
  group_by(chapter) %>%
  summarise(sentiment_score = mean(sentiment, na.rm = TRUE)) %>%
  mutate(method = "SenticNet")

#  SentiWordNet Sentiment per chapter 
sentiword_sentiment <- book_tokens %>%
  inner_join(sentiword_lex, by = "word") %>%
  group_by(chapter) %>%
  summarise(sentiment_score = mean(sentiment, na.rm = TRUE)) %>%
  mutate(method = "SentiWordNet")

# Combine results for plotting or further analysis
combined_sentiment <- bind_rows(sentic_sentiment, sentiword_sentiment)

# View combined sentiment
print(combined_sentiment[30:50, ], n=20)

```

# 11. PLOT EMOTIONAL ARC
```{r}

#Figure 1
ggplot(combined_sentiment, aes(x = as.numeric(chapter), y = sentiment_score, color = method)) +
  geom_point(alpha = 0.6) +
  geom_line(linewidth = 1) +
  geom_smooth(se = FALSE, method = "loess", linewidth = 1.2) +
  scale_color_manual(values = c("SenticNet" = "red", "SentiWordNet" = "seagreen3")) +
  labs(
    title = "Figure 1. Emotional Arc of 'Around the World in 80 Days'",
    subtitle = "Sentiment per chapter using SenticNet and SentiWordNet",
    x = "Chapter",
    y = "Average Sentiment Score",
    color = "Lexicon"
  ) +
  theme_minimal(base_size = 16) +
  theme(
    plot.title = element_text(face = "bold"),
    plot.subtitle = element_text(size = 13),
    legend.position = "right",
    legend.title = element_text(face = "bold")
  )

```

# 12. ADD KEY JOURNEY EVENTS TO THE EMOTIONAL ARC
```{r}

# Updated events data frame with descriptions
events <- data.frame(
  chapter = c(2, 5, 8, 15, 20, 25, 30, 36),
  sentiment_score = c(0.12, 0.10, 0.15, 0.09, 0.05, 0.04, -0.02, 0.20),
  label = LETTERS[1:8],
  description = c(
    "Fogg departs London",
    "Reaches Suez",
    "Arrives in Bombay",
    "Saves Aouda",
    "Storm in China Sea",
    "Rescues Passepartout",
    "Arrest in Liverpool",
    "Wins the wager"
  )
)

ggplot() +
  # Sentiment lines
  geom_line(data = combined_sentiment,
            aes(x = chapter, y = sentiment_score, color = method), size = 1) +
  geom_point(data = combined_sentiment,
             aes(x = chapter, y = sentiment_score, color = method), size = 2) +
  
  # Letters above event points
  geom_text(data = events,
            aes(x = chapter, y = sentiment_score, label = label),
            vjust = -1, size = 5, fontface = "bold") +
  
  # Event descriptions outside graph on the far right
  geom_text(data = events,
            aes(x = max(combined_sentiment$chapter) + 2,
                y = rev(seq(min(combined_sentiment$sentiment_score),
                            max(combined_sentiment$sentiment_score),
                            length.out = nrow(events))),
                label = paste0(label, ": ", description)),
            hjust = 0, size = 4) +
  
  scale_color_manual(values = c("SenticNet" = "#E74C3C",
                                "SentiWordNet" = "#1ABC9C")) +
  labs(
    title = "Figure 2. Emotional Arc of 'Around the World in 80 Days'",
    subtitle = "Sentiment per chapter using SenticNet & SentiWordNet",
    x = "Chapter",
    y = "Average Sentiment Score",
    color = "Lexicon"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    legend.position = "bottom",
    plot.margin = margin(t = 10, r = 150, b = 10, l = 10 ), # extra space on right
    plot.subtitle = element_text(size = 10)
  
  ) +
  coord_cartesian(clip = "off") # allow drawing outside


```

